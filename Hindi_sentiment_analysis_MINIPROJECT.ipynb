{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hindi sentiment analysis MINIPROJECT ",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MINI PROJECT BY -- DEEPANSHU UPADHYAYA\n",
        "# SECTION -- CST\n",
        "# UNIVERSITY ROLL NO -- 2014383"
      ],
      "metadata": {
        "id": "syleN45H3Wj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# importing required moules\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Tti4IUl22St"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading modules\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt, random\n"
      ],
      "metadata": {
        "id": "RbP7XtR9E8CL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string,re"
      ],
      "metadata": {
        "id": "n5GCG5HQZRz9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-WIk0eoqCcg"
      },
      "source": [
        "## download nltk module data and google translation module\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install googletrans==3.1.0a0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iCgkS6r_EJf",
        "outputId": "307d139f-5995-48b0-dccd-26faf21d7e0e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.7/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.6.15)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.12.1)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDM1bCwF82f1",
        "outputId": "3a093307-15d2-4b4e-e7e5-24cec1bb406d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO5Oa0ly_LTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abca7128-01ab-4212-fff2-cfc4e1e217fd"
      },
      "source": [
        "\n",
        "foo_nltk=__import__('nltk')\n",
        "foo_nltk.download('stopwords')\n",
        "foo_nltk.download('punkt')\n",
        "foo_nltk.download('averaged_perceptron_tagger')\n",
        "foo_nltk.download('tagsets')\n",
        "foo_nltk.download('treebank')\n",
        "foo_nltk.download('wordnet')\n",
        "foo_nltk.download('twitter_samples')\n",
        "\n",
        "del foo_nltk"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load data "
      ],
      "metadata": {
        "id": "PrRLoVPbRies"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data loader \n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "# get some random index list \n",
        "random_indexs=np.random.randint(0,high=5000,size=5)"
      ],
      "metadata": {
        "id": "bmLTP8XPVuLX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tweets_data=twitter_samples.strings(fileids='tweets.20150430-223406.json')\n",
        "# total number of tweets \n",
        "print('Total number of smaples:',len(tweets_data))\n",
        "# see random tweets (any five)\n",
        "for index in np.random.randint(0,high=len(tweets_data),size=5):\n",
        "  # print random tweet\n",
        "  print('Tweet number:',index,':',tweets_data[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGSDqwXTWp7f",
        "outputId": "f296b659-3b44-4588-98f9-5e220bd99d6a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of smaples: 20000\n",
            "Tweet number: 3579 : RT @AamerAnwar: What is the purpose of Willie Rennie? Serious question #SNP #VoteSNP\n",
            "Tweet number: 15545 : RT @BBCDouglasF: Sturgeon #QE2015: a lot of her 30mins on the back foot over #indyref2 + health questions. Lacked usual clarity of key mess…\n",
            "Tweet number: 5305 : RT @SkyNews: THE INDEPENDENT FRONT PAGE: \"Miliband: I will not do deal with the SNP to become PM\" #skypapers http://t.co/im5LfHEUij\n",
            "Tweet number: 9044 : RT @andrewducker: Can someone explain to me what Ed Miliband is thinking? http://t.co/Esup0BLBrP\n",
            "Tweet number: 6255 : RT @LabourLeft: \"Clare Gerada quits NHS England to be able to speak out on Tories' NHS privatisation\" via Pulse Today: http://t.co/oRGDAftc…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions "
      ],
      "metadata": {
        "id": "iIkRdC8MZ8z6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data cleaning functions "
      ],
      "metadata": {
        "id": "AAKZYgklrSvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make remove - \"# tag\" and \"@ tag\" strings\n",
        "def tagRemover(data:list,to_remove:list=['#[\\S]*','@[\\S]*','http[\\S]*'],\n",
        "               filler=''):\n",
        "  for pattern in to_remove:\n",
        "    data=[re.sub(pattern,filler,tweet) for tweet in data] \n",
        "  return data \n",
        "\n",
        "# function to get set of similes in data\n",
        "def getSimiles(data:list,pattern:str='[a-zA-Z0-9]*',filler=''):\n",
        "  # make set of all similes used in twitter data \n",
        "  similes=set()\n",
        "  for tweet in data:\n",
        "    for simile in re.sub(pattern,filler,tweet).split(): \n",
        "      similes.add(simile)\n",
        "  return similes\n",
        "\n",
        "# function to remove special characters\n",
        "def specialCharaterRemover(data:list,specialInData:set,to_leave:dict,\n",
        "                           decodeLeaves:dict,filler=' '):\n",
        "  #revmove all characters except alphabets\n",
        "  to_remove=string.printable.replace(string.ascii_letters,'')\n",
        "  to_remove_trans=str.maketrans(to_remove,filler*len(to_remove))\n",
        "  # join all special characters in data - get all special characters in twitter data  \n",
        "  special_charters=''.join(specialInData)\n",
        "  # leave all special in special characters\n",
        "  for key,value in to_leave.items():\n",
        "    special_charters=special_charters.replace(key,value)\n",
        "  special_charters_trans=str.maketrans(special_charters,filler*len(special_charters))\n",
        "  data=[tweet.translate(special_charters_trans) for tweet in data]\n",
        "  # convert ':(' or ':)' to word \"negative\" or \"postive\" \n",
        "  for key,value in decodeLeaves.items():\n",
        "    data=[tweet.replace(key,value) for tweet in data]\n",
        "  data=[tweet.translate(to_remove_trans) for tweet in data]\n",
        "  return data\n",
        "\n",
        "# load nltk tokenizer \n",
        "from nltk import word_tokenize\n",
        "# load stopwords \n",
        "from nltk.corpus import stopwords  \n",
        "#removing stopwords and words having length less than three\n",
        "def removeStopWords(data:list,stopwords_list:list=stopwords.words(),minWordLen=2):\n",
        "  # get list of all stopwords (of all languages)\n",
        "  data2return=list()\n",
        "  # clean tweets (after tokenizing)\n",
        "  for tweet in data:\n",
        "    data2return.append([word for word in word_tokenize(tweet) if (word not in stopwords_list) and len(word)>minWordLen])\n",
        "  return data2return\n",
        "  "
      ],
      "metadata": {
        "id": "27irYXEtZ2fk"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pre-processing functions "
      ],
      "metadata": {
        "id": "NSUzC5PrrYvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load Stemmer and Lemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# function to stem and lemmatize word \n",
        "def wordCleaner(data:list):\n",
        "  # make Stemmer\n",
        "  stemmer_=SnowballStemmer('english')\n",
        "  # make lemmatizer\n",
        "  lemmatizer_=WordNetLemmatizer()\n",
        "  cleanedData=list()\n",
        "  \n",
        "  # make clean - sentence by sentence \n",
        "  for sentence in data:\n",
        "    # add pos tags (and clean word by word)\n",
        "    # make list of cleaned words \n",
        "    cleanSentences=list()\n",
        "    for token,tag in pos_tag(sentence):\n",
        "      # if tag start with NN\n",
        "      if tag.startswith('NN'): \n",
        "        tag='n' # make noun \n",
        "      # if tag start with VB\n",
        "      elif tag.startswith('VB'): \n",
        "        tag='v'  # verb\n",
        "      \n",
        "      else: \n",
        "        tag='a'\n",
        "      # apply Lemmatization \n",
        "      token=lemmatizer_.lemmatize(token.lower(),tag)\n",
        "      # apply stemmer \n",
        "      token=stemmer_.stem(token)\n",
        "      # apped words to clean sentences list \n",
        "      cleanSentences.append(token)\n",
        "    # append cleaned stentences to cleaned data list \n",
        "    cleanedData.append(cleanSentences)\n",
        "\n",
        "  return cleanedData\n",
        "\n",
        "# function make list to generator \n",
        "def getGenerator(data:list):\n",
        "  for sentence in data:\n",
        "    for token in sentence:\n",
        "      yield token \n",
        "\n",
        "# function to get modeled token ductionary like -> {'word':True,'python':True}\n",
        "def getModeledTokens(data:list):\n",
        "  # get sentence from data \n",
        "  for sentence in data:\n",
        "    yield dict([(token,True) for token in sentence])"
      ],
      "metadata": {
        "id": "Hhr66DVUrh6A"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## view data "
      ],
      "metadata": {
        "id": "vpsGK9MekU6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# positive tweets \n",
        "positive_tweets=twitter_samples.strings('positive_tweets.json')\n",
        "# total number of positive tweets\n",
        "print('Total number of positive smaples:',len(positive_tweets))\n",
        "# see random tweets\n",
        "for index in random_indexs:\n",
        "  print('Tweet number:',index,':',positive_tweets[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnUxDgSFTTjt",
        "outputId": "8e8ff645-ac78-40c0-fda1-d77c56b28250"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of positive smaples: 5000\n",
            "Tweet number: 327 : Gratefulness = Happiness :) Post three things you're grateful for today in the comments\n",
            "Tweet number: 554 : @zara_arsalan thnks progrmr :-)\n",
            "Tweet number: 4084 : @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
            "Tweet number: 61 : @MacatangayApril  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)\n",
            "Tweet number: 3728 : @iamBugti_18 &lt;------ff__special\n",
            "Acha banda hai ✌ tweets bhi achi krta hai\n",
            "Handsome bhi hai 😂\n",
            "Must follow :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#negative tweets \n",
        "negative_tweets=twitter_samples.strings('negative_tweets.json')\n",
        "# total number of positive tweets\n",
        "print('Total number of negative smaples:',len(negative_tweets))\n",
        "# see random tweets \n",
        "for index in random_indexs:\n",
        "  print('Tweet number:',index,':',negative_tweets[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSJoxROFU5Eh",
        "outputId": "6ef433e5-b720-470b-994e-6a71d4fb5d7c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of positive smaples: 5000\n",
            "Tweet number: 327 : Such a stressful and upsetting day yesterday, the UK government sucks :(\n",
            "Tweet number: 554 : It just ruins your #FridayFeeling when you know next day you have to wake up early in the morning....:(\n",
            "Tweet number: 4084 : Wish that I can buy all Bangtan's merch :(\n",
            "Tweet number: 61 : @Mess0019 Well I am sure your work day is over before mine :(\n",
            "Tweet number: 3728 : @vinrana1986 hii vin plss rply my tweet :((\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clean data "
      ],
      "metadata": {
        "id": "d07BRVrlkfc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## make remove - \"# tag\" and \"@ tag\" strings\n",
        "# apply to all positive strings \n",
        "cleaned_positive_tweets=tagRemover(positive_tweets)\n",
        "# apply to all negative strings \n",
        "cleaned_negative_tweets=tagRemover(negative_tweets)\n",
        "\n",
        "#Finding similes used in data\n",
        "\n",
        "#make set of all similes used in positive twitter data \n",
        "positive_similes=getSimiles(cleaned_positive_tweets)\n",
        "#total positive similes found \n",
        "print('Total positive similes found:',len(positive_similes))\n",
        "# see all found positive similies \n",
        "print('Positive similes:',positive_similes)\n",
        "\n",
        "# make set of all similes used in negative twitter data \n",
        "negative_similes=getSimiles(cleaned_negative_tweets)\n",
        "# count total negative similes found \n",
        "print('Total negative similes found:',len(negative_similes))\n",
        "# see all found negative similies \n",
        "print('Negative similes:',negative_similes)\n",
        "\n",
        "# do hand picking for positive similes \n",
        "positive_similes_code={':)':' positive ',\n",
        "                       ':D':' positive ',\n",
        "                       ':p':' positive '}\n",
        "\n",
        "# do hand picking for negative similes \n",
        "negative_similes_code={':(':' negative '}    \n",
        "\n",
        "## clean tweet data - remove and decode smilies \n",
        "to_prevent_positive={':':'',')':''}\n",
        "# clean positive data \n",
        "cleaned_positive_tweets=specialCharaterRemover(cleaned_positive_tweets,positive_similes,to_prevent_positive,\n",
        "                                               positive_similes_code)\n",
        "# clean negative data\n",
        "to_prevent_negative={':':'','(':''}\n",
        "cleaned_negative_tweets=specialCharaterRemover(cleaned_negative_tweets,negative_similes,to_prevent_negative,\n",
        "                                               negative_similes_code)\n",
        "\n",
        "## make tokenize data \n",
        "# positive tweets \n",
        "cleaned_positive_tweets=removeStopWords(cleaned_positive_tweets)\n",
        "# negative tweets \n",
        "cleaned_negative_tweets=removeStopWords(cleaned_negative_tweets)"
      ],
      "metadata": {
        "id": "tLsMGxBOZ9mF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e8cf2c-4cc5-4e94-8e1a-6a4de96117f5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total positive similes found: 496\n",
            "Positive similes: {':))', '¦', ':-)', '^-^', '((', '.........', '?-', '“”', '🇫🇷', '\":)', '!!..;', '^^', '🐶', '♥♥', '«»', '★★★★', '(', '.:-)', '💜', '((-:', '/-\\\\', ':(', ':):)', '´', '_', '☺', ':-).', '...!!!', '||', \"'.\", '💓', '-!!', ',', '?:)', ':))😌', \"'!!\", '😤😤', '✧｡', '→', '-_-', '💞😘', '..!!!', ':)():)!:!;', '😂😁', '🔴', ':)💞', ']', '\"-\"', '-', '|', ':)???', '=', '-_', '\"-', '👊🏻“', '../', '😂😂', '?(', '.!', '(?)', '😉', '🙈', '`', '__', '?!?!?!', '&;:&;', '★', '˚', '!!!!:=.', '!:)', '/??\"\"', '=^.^=', '*', '£.', '))', '-?', '⅛', '...:)!!', '/!', '$$,', '😒🌚', '$!!', '!!!', 'හවුල', '^', '.&;', '!\"', '\\\\:)/', ':::::)', ':))))))))))))))))))))))))))', \"''''\", '🍹🍸🍻', '\\\\', ';__;', \"??'\", '(!:)).', '♡♥♡', '-.:-)', \"'...\", '/', '♡♥♡♥', '...!!', '_:', '---:', '🍸╲──╱🍤╭╮╭╮┓┓╭╮╮┳╭🍸╲──╱🍤', '..\"', '!?!', ':*', ':))):):)', '❤', \"')\", '👈💗', '!:)😘', '???', '$,', '😧', '?💕', ':).=', '!!!!', '&;&;', '!”', '::', '--&;', '[', '❤👓💘', '.....:-)', ':)💘', '👱🏽👽🌳', 'ęÿ', '💪', ':):):):):)', '=:', '♌️', '!!!!!!!!!!', '\"', '&;------__', ':)))))))))', '-&;', ':)🎉🎈', '.....', '•', ',?', '&;', ':)', '&;!', '👈💖', '❤️😘', '&;/', \"',\", '♬', '❤💙💚💕❤💙💚💕', '🎉🎉🎉', '!!!:)', ',\"', '—', '...........', '👌\"', ':-)&;', ':-', '&;.&;', '😒', '%', ':-)\"\"', ':)❤💜💘💋', ',,,…', ':)!!!', '😎💁🏻', '~!', '!!', ';))', '!!:)', '·', ';)))', ':!', ':\"&;', ':),', '✌', ':)));))))', '&;---', '!!!!!', \"''.\", '\":', '//', 'හෙල', '....:', '♡♡♡♡', '--', '??:-)', '\"\".', '👏', '..-:))', '!😂', '💟💟', '☺☺', ').', '---', \",'\", ':)\\\\(^.^)/', '\\\\/', '\\U000fec00', ':)&;', '°', ':))))))', '.:)☺️', '^_^', '?:)))', '{}', 'العنزي,', ':))))))))', ':))((', '♡', ':&;', '...:-)', 'ä', '😍😊', '...?', '&;,,&;!!', '..:)', 'للعودة', '!!:-)', ':)))', '~!!', ':-))))', '(/)', '😃', '...♬', \"'',\", ':)!', ':):):)', '^.^', '!!!!!!!!', '🙆🙌😛😄😄😁😂😂🐒😁😝❤', ')', ':)))))))', '“', '😊', '&;;', '====&;', ':))))))))))))))))))', '!!!)', '😀😁😂😃😄😅😉', '??:)', '!!?', '.', '–', '...:)', '🍅:', '?💗', '👊', '🍰', '(é)', \"...'\", ';):*', \"*...'\", '!..:)', ':)):))):)))', ':-)))))', \"'\", '�', '…', '....', ':-?', '🔥', ';', '-:)', '🙅', '👍🏼👍🏼👍🏼😍😍😍😍😍🌸🌸🌸🌸🌸', ')))))', '!', '..', '💟', ':))))))))))', ':))))', '……', '/?', '✨✨✨', '👈😘', ':)…', '▪️', '....:)', 'øÔ', '/.', '♥', ':))))))))))))', '?!', '£.,', '🍤─╲╱─🍸┣╱╰╯┗┗╰╯╰┻╯🍤─╲╱─🍸', '❤❤', ':', '😘😘', '😊😊', ':-)..', '&;:)', '🚂', '?!!\"', '💖💖:)', '!!!!!!!!!', '??!!', 'بندر', '!*', '(.', '/??\"', '\"...', '~', ':))\"', ':!!!!', ':|', ':))))!!!!!!', '????', 'ūō-', ';*!', '｡✧', '()/.', '---&;', '..:-)', '....?', '♥:)', '\"\"', '.\"', '-!', ':)”\"', '***,', '*...', '(...)', ':-)))))))', '✫', '++', '😷', '-⅞', '🎵🎵🎵🎵', \":)'\", '☕☕☕', '👈🔥', '/:', '!:))', '🇫🇷🍷🍸☀️🇫🇷🚲🇫🇷🍷🍸☀️🚲', '👈💓', ':)))))', '\",', \"''\", '✧', '.....:)', ':):):):)', '______.', '😄😄?', '........', '!!!!!!', '*-*', ':\")', \"........'\", '!:-)', ':)))))))))))))))', '⊹', '(&;', '-!!:)', '/,', '👉', '♫', '👍', '🎉', '?', '?\"', '❤️', ':).', '😂😂😂', ':)\"\"', '✵', ':-*', '?.', '(:', '😘', '?😘😘😘', '-.', '💃💃💃\"', ':-))', '😴', '♡♡♡♡♡', ':-)...', ',,,', '()', '⚡', '&;:', '🐑', '💅🏽💋', '?:))', '\\U000fe334', '❤️❤️', '☆', '”', '..:', '?!?', '👯', '::))', '):', ';)', \".'\", \":')\", '😜', ';-))', '................', '😁', '😀', '(..', '...*', ':/', \"'..\", ':::::::', ':-)))', '££,', '():', '😭❤️', '\"\\'', '~😊', '/-', '«--\"', ';-)', '🐢', ',,', '😭', \"'?\", '**', '😂', 'للحياة,', '......', ':-):-))', ':)..', ':\"', ':-)\"', '!,', '∞', '🌷', \"''!\", '???\"', '💕', \"'!\", '🌞🌞🌞', ':---))', '...:', '.:', '_____________', '+', '!:)))', '?:', '..,', ':...', '’', '$', '//*', ':)...', '✨', ':-):-)', '!!!!!!!!!!!!!!', '\"\",', 'ź', \"?'\", ':.', '!!..', '.!!.', ':)♡', ':-)(-:', ':-))\"', '💜💜💜💜', ':):', '⭐', '∗∗', \"'!:)\", '😊😊😊', \":'-)\", ':)\"', '!!!!:)', '.-..,', '[]', '£', '💗', 'é', '🍭', '←', '💓🚮👯-', '🌎🌎❤️❤️❤️', '??', '-,', '😍', '🎂', '.:)', '=!', '::)', '.....!!!!', '%!!', '♥★☆', ':-))))))))', '.......', '???:', ':)❤️', '!:', '£......:)', '...'}\n",
            "Total negative similes found: 446\n",
            "Negative similes: {';(', 'फिर', ':(.', '..-..', '.........', ',,,,&;', '✈️', '\"...?', ':(\"', '^^', ':((((', '.........,', '💞', '?:((', ':-((', '??:(', '(', '(!!)', '💜', ':(', '):(\"', ':-((((((', '.+)', '$.', '´', '_', '🍜🍜🍜🍜🍜', '!!!...', '☺', '😂😩😩', '://:(\\\\\\\\\\\\', '💔', '||', \"'.\", '-........:(', ',', 'सेँ', \"'!!\", '😩😩😩😩😭', '...........:(', '❤❤❤', '✧｡', '💙', '→', '~~~', '?:(', '....:(', '-_-', ':((((((((((((((((((((((((', \"='(\", '!!&;:(', '...:(;(;(', '💪🏻🐒', ']', '😭😭😭😭😭😭', '\"-\"', '-', '|', '(:(', '=', '😂😂', '`', '?!?!?!', '_:(´ཀ`」', '{:-(', '😟', '....:((', '/./_', '*', '----', '-?', 'धोखा........', '♛♛♛', ':-(((((((((((', '$$,', '!?', '...&;:(', '!!!', \":'(\", ':--((((”', ':(,', '&;.', '^', ':((((((((', '?😂😊💘💘', '.:((', '������♥♥♥♥', '😔😓', ';__;', '✔️', \"':(\", \"'...\", '/', '♡♥♡♥', '...!!', ':(...', ',.', '😒\"', ':*', '‘’', '❤', '.............', '???', ':(((((((', '….!', '!...', '!!!!', '&;&;', '&;:((', '[', '\"!\"', '???:-(', '💪', '?!!', 'ñ', '\"', 'ＭＥ', ':((💦☔️', '(:-(', '&;&;&;&;&;&;&;&;&;&;', '?&;:(', '?:((((', ';_;', ':(:', ':((((((((((((((((((((((((((((((((', '?.....', '💞🎈🎂🎉😘', '.....', '♡♡♡', ':~', '&;', '😢', ':??', ':)', '🚖🗽🌃', '&;/', 'एक', \"',\", ':((((😩', '’,', ':(&;/', '—', '...........', '👌\"', '😩😩😩😢', '...❤', ':(:(:(:(', ':-', '$$$$', '&;.&;', ':(((((((((((', '%', ':(..', ':-(:-(:-(', ':-(.', \":(:'(\", '。。。_:(´ω`」', '...,', '!!', ':(((((((((((((((((', '😘🎉', '������', ';(\"', ':-(\"', ':!', '😞', '~~', '😫', '😭😭😭😭😭😭😭😭😭😭', '🙏🏻🎂', ';-;', '☺💜', '!!!!!', \"''.\", '//', ':)):):):(', '!!!!!!!💥✨💫', '😉\"', '😘💖', '\":(', ':(\"”', ':(❤️❤️', '--', '》》》》ＳＥＥ', '😘😘😘😘', ':-(”', '.....:', ':----((((((', '💛💛💛💛', ').', ':(💜', '---', '&;&;:(', '😪😪😪', '.…', \",'\", '😺💒💎🎉', '.????', '?????', '😭🇺🇸', '😚', '(((', ':-(', '**.', '^_^', '\\U000fe196', '...:(', 'é’', ':´(', '.....:(', '♡', ':&;', '...?', '∠):_', ':(((((', '😬', '😊😊😊❤️', '&;:(', '(/)', '😏😏\"', '!!!!!!!!', ')', '/?:((', '“', '😊', '������♥♥', '^_^...', '����\"', '.', '☺️😂😁', '–', '?:(\"', '.-', \"...'\", '😕:-(', '!:(', ';;', \"'\", 'äø', '.:-(', ':(…', '😆😆', '…', '______', '....', ':-(☔️', ':(\"\"', ':(?', ':(((((((((((((((', ';', '/(', '!', '..', '-...', '&;--', '🇺🇸', '/?', '😩', '😊💞', '/.', '😀\"', '♥', '?!', '&;:-(', '😀🔫', ':', '😘😘', \"á'\", ':((((((((((', '?.....:((((', \"..'\", 'Фотосет:', '🎀', ':((((((((((((', '?😊', '..!!', ':(((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((', '💗💗💗', '\"...', ':(((((((((', ':(:(', '~', '😂😩', ':(((((((((((((', \":'\", ':|', '》》》》', '????', 'تكفى', '｡✧', '\"\\'\"', ':(((', '😭😭', '.:(', '💕💕💕💕', '............./', '😋', '\"\"', ':(;(', '.\"', '-!', '😿💓💓💕😚😚😚😚😚❤️❤️❤️❤️❤️', \"!!!'\", '😊”', '😔😔😔', '?!?!!', ':((((((((((((((((((((((((((((((((((((((((((((((((((((((((((', '++', ':-(((', '^__________^', ':((', '😂💞', '😷', ':-(((((((((', '\\\\\"', '＠_', '......:((', '😄👍', '\",', \"''\", '😓', '＠__', ':(;', '?!?!?:(:(:(:(:(', '(,', '🍹🍸🍹', '........', '!!!!!!', ':..', '💋❤💋❤', '..:-(', '!!!!!!!!!!!!', '¿', '😒😒', '👉', '??!', ':(:(:(', '♫', '?', '?\"', '������♥♥♥', '❤️', '😩😭🍵🌸', '💖', '((((', '😂😂😂', '🌞💃🏽🍹', '👆🏻👆🏻', '😝', ':-*', '(:', '😘', '-.', '������♥', ':-((((', ',,,', '()', ':(😩💖', '”', '😭😭😭', \"....'\", \"~'\", \".'\", '--:', '...:(...', '😔', '😏😏', '😁', '😀', ':((((((((((((((', '...*', ':/', '❤️.', '☼', ':((:(', '\"\\'', '),', ';-)', ',,', '!(:♡♡', '😭', '**', '😂', \"'?'\", '......', '!!!).', ':(!', '👣', '..:(', '???\"', '😢😵😭', '💕', 'बार', '&;:(((((((((', '....\"\"', '...:', '+=', ':\\\\', '.:', '\"...*..', '💛', '+', '🍵☕️', ':()', ':(”', '🚙💚', ':...', '’', '$', '♥♥♥', \"('\", '(+)', '&;&;&;\"', '??!?:(', '*%*', '!!!!!!!!!!!!!!', '...:-(', ':-(:-(:-(....', ':((*', '😂😂😂\"', ':-(((((((', ':-(((((', '….', '😭👑', '[]', '😭💖', '£', '💗', 'é', ':((((((', '.”', '??', '-,', '💜💜💜', '😢\"', '😍', '*\"', '...???', ':(!!!', '.......', '.%', 'İ', ':-\\\\', \"&;'\", '&;_&;', '...'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see change in positive samples \n",
        "for index in random_indexs:\n",
        "  # print random tweet\n",
        "  print('Tweet number:',index,':',cleaned_positive_tweets[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhzGC7U9lThf",
        "outputId": "213f97af-a0ce-4844-d8f0-9ea9840f20f2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet number: 327 : ['Gratefulness', 'Happiness', 'positive', 'Post', 'three', 'things', 'grateful', 'today', 'comments']\n",
            "Tweet number: 554 : ['thnks', 'progrmr']\n",
            "Tweet number: 4084 : ['Not', 'sure', 'would', 'good', 'thing', 'bottom', 'daring', 'say', 'Miss', 'gon', 'stubborn', 'mouth', 'soaping', 'positive']\n",
            "Tweet number: 61 : ['follow', 'amp', 'follow', 'back', 'positive']\n",
            "Tweet number: 3728 : ['special', 'Acha', 'banda', 'tweets', 'bhi', 'achi', 'krta', 'Handsome', 'bhi', 'Must', 'follow', 'positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see change in negative samples \n",
        "for index in random_indexs:\n",
        "  # print random tweet\n",
        "  print('Tweet number:',index,':',cleaned_negative_tweets[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmcnomJ9leKR",
        "outputId": "b54361a7-8df0-4227-db1f-b5247d63c09b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet number: 327 : ['Such', 'stressful', 'upsetting', 'day', 'yesterday', 'government', 'sucks', 'negative']\n",
            "Tweet number: 554 : ['ruins', 'know', 'next', 'day', 'wake', 'early', 'morning', 'negative']\n",
            "Tweet number: 4084 : ['Wish', 'buy', 'Bangtan', 'merch', 'negative']\n",
            "Tweet number: 61 : ['Well', 'sure', 'work', 'day', 'negative']\n",
            "Tweet number: 3728 : ['hii', 'vin', 'plss', 'rply', 'tweet', 'negative']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pre-process data "
      ],
      "metadata": {
        "id": "DCW1Z2S5w5Hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make stem and lemmatize words (positive data)\n",
        "cleaned_positive_tweets=wordCleaner(cleaned_positive_tweets)\n",
        "# make stem and lemmatize words (positive data)\n",
        "cleaned_negative_tweets=wordCleaner(cleaned_negative_tweets)"
      ],
      "metadata": {
        "id": "LOWeSbGEw7Kp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see change in positive samples \n",
        "for index in random_indexs:\n",
        "  # print random tweet\n",
        "  print('Tweet number:',index,':',cleaned_positive_tweets[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4O9tJTjxR5K",
        "outputId": "5a084d6f-1ae9-4983-af12-aec1fe940074"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet number: 4427 : ['overwhelm', 'groupmat', 'thank', 'group', 'get', 'recit', 'posit']\n",
            "Tweet number: 1657 : ['hell', 'yes', 'will', 'donington', 'park', 'hop', 'see', 'team', 'season', 'our', 'posit']\n",
            "Tweet number: 4433 : ['mont', 'doa', 'header', 'posit']\n",
            "Tweet number: 1503 : ['check', 'place', 'avail', 'colour', 'aggreg', 'work', 'tend', 'fade']\n",
            "Tweet number: 4968 : ['thank', 'mention', 'happi', 'friday', 'posit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see change in negative samples \n",
        "for index in random_indexs:\n",
        "  # print random tweet\n",
        "  print('Tweet number:',index,':',cleaned_negative_tweets[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUxo0Dfmxk1M",
        "outputId": "f862fc1b-fc11-44e0-fa9b-d9fded4e0f17"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet number: 327 : ['such', 'stress', 'upset', 'day', 'yesterday', 'govern', 'suck', 'negat']\n",
            "Tweet number: 554 : ['ruin', 'know', 'next', 'day', 'wake', 'earli', 'morn', 'negat']\n",
            "Tweet number: 4084 : ['wish', 'buy', 'bangtan', 'merch', 'negat']\n",
            "Tweet number: 61 : ['well', 'sure', 'work', 'day', 'negat']\n",
            "Tweet number: 3728 : ['hii', 'vin', 'plss', 'rpli', 'tweet', 'negat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poistiveGenerator=getGenerator(cleaned_positive_tweets)\n",
        "print(f'type of postive_word_gen_ -> {poistiveGenerator}:',type(poistiveGenerator))\n",
        "# make frequency dict \n",
        "from nltk import FreqDist\n",
        "# make postive words frequency dictionary \n",
        "positive_words_frequency=FreqDist(poistiveGenerator)\n",
        "# see -> frequency dict \n",
        "print(positive_words_frequency)\n",
        "print('Frequency of top 10 most common elements [positive]:\\n',positive_words_frequency.most_common(10),sep='')\n",
        "type(positive_words_frequency) # to see resutl -> postive_freq_dict_ -> return dict "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j29lxDNzJZo",
        "outputId": "4e10e3c5-980d-4277-f0e0-ce9bb0bdc846"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type of postive_word_gen_ -> <generator object getGenerator at 0x7f66c63564d0>: <class 'generator'>\n",
            "<FreqDist with 5227 samples and 29482 outcomes>\n",
            "Frequency of top 10 most common elements [positive]:\n",
            "[('posit', 4509), ('thank', 646), ('follow', 443), ('love', 399), ('good', 288), ('get', 273), ('day', 248), ('like', 233), ('happi', 208), ('see', 196)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.probability.FreqDist"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negativeGenerator=getGenerator(cleaned_negative_tweets)\n",
        "print(f'type of postive_word_gen_ -> {negativeGenerator}:',type(negativeGenerator))\n",
        "# make frequency dict \n",
        "from nltk import FreqDist\n",
        "\n",
        "# make postive words frequency dictionary \n",
        "negative_words_frequency=FreqDist(negativeGenerator)\n",
        "# see -> frequency dict \n",
        "print(negative_words_frequency)\n",
        "print('Frequency of top 10 most common elements [negative]:\\n',negative_words_frequency.most_common(10),sep='')\n",
        "type(negative_words_frequency) # to see resutl -> postive_freq_dict_ -> return dict "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW1I7B_uxlJ0",
        "outputId": "5e5c05a6-4dfd-401b-b2ed-ced1e4f74dc8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type of postive_word_gen_ -> <generator object getGenerator at 0x7f66be3744d0>: <class 'generator'>\n",
            "<FreqDist with 5349 samples and 28399 outcomes>\n",
            "Frequency of top 10 most common elements [negative]:\n",
            "[('negat', 4636), ('get', 330), ('miss', 304), ('pleas', 275), ('follow', 263), ('like', 223), ('time', 166), ('feel', 158), ('love', 151), ('day', 150)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.probability.FreqDist"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make dataset for model (modeled data)\n",
        "poistive_words4Data=getModeledTokens(cleaned_positive_tweets)\n",
        "negative_words4Data=getModeledTokens(cleaned_negative_tweets)\n",
        "# see (first sample)\n",
        "print('Sample: ',next(getModeledTokens(cleaned_positive_tweets)))\n",
        "\n",
        "# make dataset - \n",
        "positive_dataset=[(tweet_dict,'Positive') for tweet_dict in poistive_words4Data]\n",
        "negative_dataset=[(tweet_dict,'Negative') for tweet_dict in negative_words4Data]\n",
        "# merge dataset \n",
        "dataset=positive_dataset+negative_dataset\n",
        "print('Size of dataset:',len(dataset)) \n",
        "# split data set -> 70:30 \n",
        "from sklearn.model_selection import train_test_split\n",
        "# split data\n",
        "train_set,test_set=train_test_split(dataset,test_size=0.30)\n",
        "# see size \n",
        "len(train_set),len(test_set) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4S64pxhzou6",
        "outputId": "5fc72446-e8a5-4f7c-f911-ae22353df022"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample:  {'top': True, 'engag': True, 'member': True, 'communiti': True, 'week': True, 'posit': True}\n",
            "Size of dataset: 10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7000, 3000)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see first sample (final data)\n",
        "[(tweet_dict,'Positive') for tweet_dict in getModeledTokens(cleaned_positive_tweets)][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5I8A1vn1RoL",
        "outputId": "fd82f7ca-8a52-44ec-e48a-0756e188f3a9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'communiti': True,\n",
              "  'engag': True,\n",
              "  'member': True,\n",
              "  'posit': True,\n",
              "  'top': True,\n",
              "  'week': True},\n",
              " 'Positive')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKkxzghUXV-d"
      },
      "source": [
        "## training and testing model - By using NaiveBayesClassifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um-IOKZNUQeR",
        "outputId": "cb5c7be8-230f-4391-8cf8-d6749f778961"
      },
      "source": [
        "# import module \n",
        "from nltk import classify,NaiveBayesClassifier\n",
        "# train maodel \n",
        "classifier_=NaiveBayesClassifier.train(train_set)\n",
        "# test model \n",
        "print('Accuracy score:',round(classify.accuracy(classifier_,test_set)*100,3))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score: 95.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2oFK3VUoQAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d5405ff-ca0f-4029-d30e-9fb7b7917ddb"
      },
      "source": [
        "# classifier 10 most informative features\n",
        "classifier_.show_most_informative_features(10)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "                   negat = True           Negati : Positi =   2116.7 : 1.0\n",
            "                   posit = True           Positi : Negati =   1990.9 : 1.0\n",
            "                     bam = True           Positi : Negati =     21.9 : 1.0\n",
            "                     sad = True           Negati : Positi =     20.3 : 1.0\n",
            "                     too = True           Negati : Positi =     16.5 : 1.0\n",
            "                 appreci = True           Positi : Negati =     15.4 : 1.0\n",
            "                     via = True           Positi : Negati =     15.4 : 1.0\n",
            "              bestfriend = True           Positi : Negati =     15.1 : 1.0\n",
            "                  awesom = True           Positi : Negati =     13.6 : 1.0\n",
            "                 definit = True           Positi : Negati =     12.8 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGBl-rJEX8hi"
      },
      "source": [
        "## testing on hindi input"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conversion(get_inp):\n",
        "  from googletrans import Translator\n",
        "  translator = Translator()\n",
        "  get_inp = translator.translate(text = get_inp,dest = 'en',src = 'hi')\n",
        "  get_inp = get_inp.text\n",
        "  return [get_inp]"
      ],
      "metadata": {
        "id": "D9N-Yo3I9aZe"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample postive sentence \n",
        "inp=input(\"Enter text in hindi\")\n",
        "custom_tweet=conversion(inp)\n",
        "cleaned_custom_tweet=removeStopWords(specialCharaterRemover(tagRemover(custom_tweet),positive_similes,\n",
        "                                                            to_prevent_positive,positive_similes_code))\n",
        "# preprocessing - \n",
        "custom_tokens=wordCleaner(cleaned_custom_tweet)\n",
        "\n",
        "# print results ## Note classifier_ takes only single sample at a time (not like sklearn classifiers) \n",
        "print(custom_tweet,' <- type:',classifier_.classify([dict([token,True] for token in sample) for sample in custom_tokens][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwmfV0_t3af8",
        "outputId": "7fdf1d6e-7938-473e-d8e6-6cd4ad4e3116"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text in hindiभारत सुंदर है\n",
            "['India is beautiful']  <- type: Positive\n"
          ]
        }
      ]
    }
  ]
}