{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hindi sentiment analysis MINIPROJECT ",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MINI PROJECT BY -- DEEPANSHU UPADHYAYA\n",
        "# SECTION -- CST\n",
        "# UNIVERSITY ROLL NO -- 2014383"
      ],
      "metadata": {
        "id": "syleN45H3Wj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# importing required moules\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Tti4IUl22St"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading modules\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt, random\n"
      ],
      "metadata": {
        "id": "RbP7XtR9E8CL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string,re"
      ],
      "metadata": {
        "id": "n5GCG5HQZRz9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-WIk0eoqCcg"
      },
      "source": [
        "## download nltk module data and google translation module\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install googletrans==3.1.0a0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iCgkS6r_EJf",
        "outputId": "307d139f-5995-48b0-dccd-26faf21d7e0e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.7/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.6.15)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.12.1)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDM1bCwF82f1",
        "outputId": "3a093307-15d2-4b4e-e7e5-24cec1bb406d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO5Oa0ly_LTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abca7128-01ab-4212-fff2-cfc4e1e217fd"
      },
      "source": [
        "\n",
        "foo_nltk=__import__('nltk')\n",
        "foo_nltk.download('stopwords')\n",
        "foo_nltk.download('punkt')\n",
        "foo_nltk.download('averaged_perceptron_tagger')\n",
        "foo_nltk.download('tagsets')\n",
        "foo_nltk.download('treebank')\n",
        "foo_nltk.download('wordnet')\n",
        "foo_nltk.download('twitter_samples')\n",
        "\n",
        "del foo_nltk"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load data "
      ],
      "metadata": {
        "id": "PrRLoVPbRies"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data loader \n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "# get some random index list \n",
        "random_indexs=np.random.randint(0,high=5000,size=5)"
      ],
      "metadata": {
        "id": "bmLTP8XPVuLX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tweets_data=twitter_samples.strings(fileids='tweets.20150430-223406.json')\n",
        "# total number of tweets \n",
        "print('Total number of smaples:',len(tweets_data))\n",
        "# see random tweets (any five)\n",
        "for index in np.random.randint(0,high=len(tweets_data),size=5):\n",
        "  # print random tweet\n",
        "  print('Tweet number:',index,':',tweets_data[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGSDqwXTWp7f",
        "outputId": "f296b659-3b44-4588-98f9-5e220bd99d6a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of smaples: 20000\n",
            "Tweet number: 3579 : RT @AamerAnwar: What is the purpose of Willie Rennie? Serious question #SNP #VoteSNP\n",
            "Tweet number: 15545 : RT @BBCDouglasF: Sturgeon #QE2015: a lot of her 30mins on the back foot over #indyref2 + health questions. Lacked usual clarity of key messâ€¦\n",
            "Tweet number: 5305 : RT @SkyNews: THE INDEPENDENT FRONT PAGE: \"Miliband: I will not do deal with the SNP to become PM\" #skypapers http://t.co/im5LfHEUij\n",
            "Tweet number: 9044 : RT @andrewducker: Can someone explain to me what Ed Miliband is thinking? http://t.co/Esup0BLBrP\n",
            "Tweet number: 6255 : RT @LabourLeft: \"Clare Gerada quits NHS England to be able to speak out on Tories' NHS privatisation\" via Pulse Today: http://t.co/oRGDAftcâ€¦\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions "
      ],
      "metadata": {
        "id": "iIkRdC8MZ8z6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data cleaning functions "
      ],
      "metadata": {
        "id": "AAKZYgklrSvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make remove - \"# tag\" and \"@ tag\" strings\n",
        "def tagRemover(data:list,to_remove:list=['#[\\S]*','@[\\S]*','http[\\S]*'],\n",
        "               filler=''):\n",
        "  for pattern in to_remove:\n",
        "    data=[re.sub(pattern,filler,tweet) for tweet in data] \n",
        "  return data \n",
        "\n",
        "# function to get set of similes in data\n",
        "def getSimiles(data:list,pattern:str='[a-zA-Z0-9]*',filler=''):\n",
        "  # make set of all similes used in twitter data \n",
        "  similes=set()\n",
        "  for tweet in data:\n",
        "    for simile in re.sub(pattern,filler,tweet).split(): \n",
        "      similes.add(simile)\n",
        "  return similes\n",
        "\n",
        "# function to remove special characters\n",
        "def specialCharaterRemover(data:list,specialInData:set,to_leave:dict,\n",
        "                           decodeLeaves:dict,filler=' '):\n",
        "  #revmove all characters except alphabets\n",
        "  to_remove=string.printable.replace(string.ascii_letters,'')\n",
        "  to_remove_trans=str.maketrans(to_remove,filler*len(to_remove))\n",
        "  # join all special characters in data - get all special characters in twitter data  \n",
        "  special_charters=''.join(specialInData)\n",
        "  # leave all special in special characters\n",
        "  for key,value in to_leave.items():\n",
        "    special_charters=special_charters.replace(key,value)\n",
        "  special_charters_trans=str.maketrans(special_charters,filler*len(special_charters))\n",
        "  data=[tweet.translate(special_charters_trans) for tweet in data]\n",
        "  # convert ':(' or ':)' to word \"negative\" or \"postive\" \n",
        "  for key,value in decodeLeaves.items():\n",
        "    data=[tweet.replace(key,value) for tweet in data]\n",
        "  data=[tweet.translate(to_remove_trans) for tweet in data]\n",
        "  return data\n",
        "\n",
        "# load nltk tokenizer \n",
        "from nltk import word_tokenize\n",
        "# load stopwords \n",
        "from nltk.corpus import stopwords  \n",
        "#removing stopwords and words having length less than three\n",
        "def removeStopWords(data:list,stopwords_list:list=stopwords.words(),minWordLen=2):\n",
        "  # get list of all stopwords (of all languages)\n",
        "  data2return=list()\n",
        "  # clean tweets (after tokenizing)\n",
        "  for tweet in data:\n",
        "    data2return.append([word for word in word_tokenize(tweet) if (word not in stopwords_list) and len(word)>minWordLen])\n",
        "  return data2return\n",
        "  "
      ],
      "metadata": {
        "id": "27irYXEtZ2fk"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pre-processing functions "
      ],
      "metadata": {
        "id": "NSUzC5PrrYvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load Stemmer and Lemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# function to stem and lemmatize word \n",
        "def wordCleaner(data:list):\n",
        "  # make Stemmer\n",
        "  stemmer_=SnowballStemmer('english')\n",
        "  # make lemmatizer\n",
        "  lemmatizer_=WordNetLemmatizer()\n",
        "  cleanedData=list()\n",
        "  \n",
        "  # make clean - sentence by sentence \n",
        "  for sentence in data:\n",
        "    # add pos tags (and clean word by word)\n",
        "    # make list of cleaned words \n",
        "    cleanSentences=list()\n",
        "    for token,tag in pos_tag(sentence):\n",
        "      # if tag start with NN\n",
        "      if tag.startswith('NN'): \n",
        "        tag='n' # make noun \n",
        "      # if tag start with VB\n",
        "      elif tag.startswith('VB'): \n",
        "        tag='v'  # verb\n",
        "      \n",
        "      else: \n",
        "        tag='a'\n",
        "      # apply Lemmatization \n",
        "      token=lemmatizer_.lemmatize(token.lower(),tag)\n",
        "      # apply stemmer \n",
        "      token=stemmer_.stem(token)\n",
        "      # apped words to clean sentences list \n",
        "      cleanSentences.append(token)\n",
        "    # append cleaned stentences to cleaned data list \n",
        "    cleanedData.append(cleanSentences)\n",
        "\n",
        "  return cleanedData\n",
        "\n",
        "# function make list to generator \n",
        "def getGenerator(data:list):\n",
        "  for sentence in data:\n",
        "    for token in sentence:\n",
        "      yield token \n",
        "\n",
        "# function to get modeled token ductionary like -> {'word':True,'python':True}\n",
        "def getModeledTokens(data:list):\n",
        "  # get sentence from data \n",
        "  for sentence in data:\n",
        "    yield dict([(token,True) for token in sentence])"
      ],
      "metadata": {
        "id": "Hhr66DVUrh6A"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## view data "
      ],
      "metadata": {
        "id": "vpsGK9MekU6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# positive tweets \n",
        "positive_tweets=twitter_samples.strings('positive_tweets.json')\n",
        "# total number of positive tweets\n",
        "print('Total number of positive smaples:',len(positive_tweets))\n",
        "# see random tweets\n",
        "for index in random_indexs:\n",
        "  print('Tweet number:',index,':',positive_tweets[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnUxDgSFTTjt",
        "outputId": "8e8ff645-ac78-40c0-fda1-d77c56b28250"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of positive smaples: 5000\n",
            "Tweet number: 327 : Gratefulness = Happiness :) Post three things you're grateful for today in the comments\n",
            "Tweet number: 554 : @zara_arsalan thnks progrmr :-)\n",
            "Tweet number: 4084 : @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
            "Tweet number: 61 : @MacatangayApril  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)\n",
            "Tweet number: 3728 : @iamBugti_18 &lt;------ff__special\n",
            "Acha banda hai âœŒ tweets bhi achi krta hai\n",
            "Handsome bhi hai ðŸ˜‚\n",
            "Must follow :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#negative tweets \n",
        "negative_tweets=twitter_samples.strings('negative_tweets.json')\n",
        "# total number of positive tweets\n",
        "print('Total number of negative smaples:',len(negative_tweets))\n",
        "# see random tweets \n",
        "for index in random_indexs:\n",
        "  print('Tweet number:',index,':',negative_tweets[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSJoxROFU5Eh",
        "outputId": "6ef433e5-b720-470b-994e-6a71d4fb5d7c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of positive smaples: 5000\n",
            "Tweet number: 327 : Such a stressful and upsetting day yesterday, the UK government sucks :(\n",
            "Tweet number: 554 : It just ruins your #FridayFeeling when you know next day you have to wake up early in the morning....:(\n",
            "Tweet number: 4084 : Wish that I can buy all Bangtan's merch :(\n",
            "Tweet number: 61 : @Mess0019 Well I am sure your work day is over before mine :(\n",
            "Tweet number: 3728 : @vinrana1986 hii vin plss rply my tweet :((\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clean data "
      ],
      "metadata": {
        "id": "d07BRVrlkfc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## make remove - \"# tag\" and \"@ tag\" strings\n",
        "# apply to all positive strings \n",
        "cleaned_positive_tweets=tagRemover(positive_tweets)\n",
        "# apply to all negative strings \n",
        "cleaned_negative_tweets=tagRemover(negative_tweets)\n",
        "\n",
        "#Finding similes used in data\n",
        "\n",
        "#make set of all similes used in positive twitter data \n",
        "positive_similes=getSimiles(cleaned_positive_tweets)\n",
        "#total positive similes found \n",
        "print('Total positive similes found:',len(positive_similes))\n",
        "# see all found positive similies \n",
        "print('Positive similes:',positive_similes)\n",
        "\n",
        "# make set of all similes used in negative twitter data \n",
        "negative_similes=getSimiles(cleaned_negative_tweets)\n",
        "# count total negative similes found \n",
        "print('Total negative similes found:',len(negative_similes))\n",
        "# see all found negative similies \n",
        "print('Negative similes:',negative_similes)\n",
        "\n",
        "# do hand picking for positive similes \n",
        "positive_similes_code={':)':' positive ',\n",
        "                       ':D':' positive ',\n",
        "                       ':p':' positive '}\n",
        "\n",
        "# do hand picking for negative similes \n",
        "negative_similes_code={':(':' negative '}    \n",
        "\n",
        "## clean tweet data - remove and decode smilies \n",
        "to_prevent_positive={':':'',')':''}\n",
        "# clean positive data \n",
        "cleaned_positive_tweets=specialCharaterRemover(cleaned_positive_tweets,positive_similes,to_prevent_positive,\n",
        "                                               positive_similes_code)\n",
        "# clean negative data\n",
        "to_prevent_negative={':':'','(':''}\n",
        "cleaned_negative_tweets=specialCharaterRemover(cleaned_negative_tweets,negative_similes,to_prevent_negative,\n",
        "                                               negative_similes_code)\n",
        "\n",
        "## make tokenize data \n",
        "# positive tweets \n",
        "cleaned_positive_tweets=removeStopWords(cleaned_positive_tweets)\n",
        "# negative tweets \n",
        "cleaned_negative_tweets=removeStopWords(cleaned_negative_tweets)"
      ],
      "metadata": {
        "id": "tLsMGxBOZ9mF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e8cf2c-4cc5-4e94-8e1a-6a4de96117f5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total positive similes found: 496\n",
            "Positive similes: {':))', 'Â¦', ':-)', '^-^', '((', '.........', '?-', 'â€œâ€', 'ðŸ‡«ðŸ‡·', '\":)', '!!..;', '^^', 'ðŸ¶', 'â™¥â™¥', 'Â«Â»', 'â˜…â˜…â˜…â˜…', '(', '.:-)', 'ðŸ’œ', '((-:', '/-\\\\', ':(', ':):)', 'Â´', '_', 'â˜º', ':-).', '...!!!', '||', \"'.\", 'ðŸ’“', '-!!', ',', '?:)', ':))ðŸ˜Œ', \"'!!\", 'ðŸ˜¤ðŸ˜¤', 'âœ§ï½¡', 'â†’', '-_-', 'ðŸ’žðŸ˜˜', '..!!!', ':)():)!:!;', 'ðŸ˜‚ðŸ˜', 'ðŸ”´', ':)ðŸ’ž', ']', '\"-\"', '-', '|', ':)???', '=', '-_', '\"-', 'ðŸ‘ŠðŸ»â€œ', '../', 'ðŸ˜‚ðŸ˜‚', '?(', '.!', '(?)', 'ðŸ˜‰', 'ðŸ™ˆ', '`', '__', '?!?!?!', '&;:&;', 'â˜…', 'Ëš', '!!!!:=.', '!:)', '/??\"\"', '=^.^=', '*', 'Â£.', '))', '-?', 'â…›', '...:)!!', '/!', '$$,', 'ðŸ˜’ðŸŒš', '$!!', '!!!', 'à·„à·€à·”à¶½', '^', '.&;', '!\"', '\\\\:)/', ':::::)', ':))))))))))))))))))))))))))', \"''''\", 'ðŸ¹ðŸ¸ðŸ»', '\\\\', ';__;', \"??'\", '(!:)).', 'â™¡â™¥â™¡', '-.:-)', \"'...\", '/', 'â™¡â™¥â™¡â™¥', '...!!', '_:', '---:', 'ðŸ¸â•²â”€â”€â•±ðŸ¤â•­â•®â•­â•®â”“â”“â•­â•®â•®â”³â•­ðŸ¸â•²â”€â”€â•±ðŸ¤', '..\"', '!?!', ':*', ':))):):)', 'â¤', \"')\", 'ðŸ‘ˆðŸ’—', '!:)ðŸ˜˜', '???', '$,', 'ðŸ˜§', '?ðŸ’•', ':).=', '!!!!', '&;&;', '!â€', '::', '--&;', '[', 'â¤ðŸ‘“ðŸ’˜', '.....:-)', ':)ðŸ’˜', 'ðŸ‘±ðŸ½ðŸ‘½ðŸŒ³', 'Ä™Ã¿', 'ðŸ’ª', ':):):):):)', '=:', 'â™Œï¸', '!!!!!!!!!!', '\"', '&;------__', ':)))))))))', '-&;', ':)ðŸŽ‰ðŸŽˆ', '.....', 'â€¢', ',?', '&;', ':)', '&;!', 'ðŸ‘ˆðŸ’–', 'â¤ï¸ðŸ˜˜', '&;/', \"',\", 'â™¬', 'â¤ðŸ’™ðŸ’šðŸ’•â¤ðŸ’™ðŸ’šðŸ’•', 'ðŸŽ‰ðŸŽ‰ðŸŽ‰', '!!!:)', ',\"', 'â€”', '...........', 'ðŸ‘Œ\"', ':-)&;', ':-', '&;.&;', 'ðŸ˜’', '%', ':-)\"\"', ':)â¤ðŸ’œðŸ’˜ðŸ’‹', ',,,â€¦', ':)!!!', 'ðŸ˜ŽðŸ’ðŸ»', '~!', '!!', ';))', '!!:)', 'Â·', ';)))', ':!', ':\"&;', ':),', 'âœŒ', ':)));))))', '&;---', '!!!!!', \"''.\", '\":', '//', 'à·„à·™à¶½', '....:', 'â™¡â™¡â™¡â™¡', '--', '??:-)', '\"\".', 'ðŸ‘', '..-:))', '!ðŸ˜‚', 'ðŸ’ŸðŸ’Ÿ', 'â˜ºâ˜º', ').', '---', \",'\", ':)\\\\(^.^)/', '\\\\/', '\\U000fec00', ':)&;', 'Â°', ':))))))', '.:)â˜ºï¸', '^_^', '?:)))', '{}', 'Ø§Ù„Ø¹Ù†Ø²ÙŠ,', ':))))))))', ':))((', 'â™¡', ':&;', '...:-)', 'Ã¤', 'ðŸ˜ðŸ˜Š', '...?', '&;,,&;!!', '..:)', 'Ù„Ù„Ø¹ÙˆØ¯Ø©', '!!:-)', ':)))', '~!!', ':-))))', '(/)', 'ðŸ˜ƒ', '...â™¬', \"'',\", ':)!', ':):):)', '^.^', '!!!!!!!!', 'ðŸ™†ðŸ™ŒðŸ˜›ðŸ˜„ðŸ˜„ðŸ˜ðŸ˜‚ðŸ˜‚ðŸ’ðŸ˜ðŸ˜â¤', ')', ':)))))))', 'â€œ', 'ðŸ˜Š', '&;;', '====&;', ':))))))))))))))))))', '!!!)', 'ðŸ˜€ðŸ˜ðŸ˜‚ðŸ˜ƒðŸ˜„ðŸ˜…ðŸ˜‰', '??:)', '!!?', '.', 'â€“', '...:)', 'ðŸ…:', '?ðŸ’—', 'ðŸ‘Š', 'ðŸ°', '(Ã©)', \"...'\", ';):*', \"*...'\", '!..:)', ':)):))):)))', ':-)))))', \"'\", 'ï¿½', 'â€¦', '....', ':-?', 'ðŸ”¥', ';', '-:)', 'ðŸ™…', 'ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ˜ðŸ˜ðŸ˜ðŸ˜ðŸ˜ðŸŒ¸ðŸŒ¸ðŸŒ¸ðŸŒ¸ðŸŒ¸', ')))))', '!', '..', 'ðŸ’Ÿ', ':))))))))))', ':))))', 'â€¦â€¦', '/?', 'âœ¨âœ¨âœ¨', 'ðŸ‘ˆðŸ˜˜', ':)â€¦', 'â–ªï¸', '....:)', 'Ã¸Ã”', '/.', 'â™¥', ':))))))))))))', '?!', 'Â£.,', 'ðŸ¤â”€â•²â•±â”€ðŸ¸â”£â•±â•°â•¯â”—â”—â•°â•¯â•°â”»â•¯ðŸ¤â”€â•²â•±â”€ðŸ¸', 'â¤â¤', ':', 'ðŸ˜˜ðŸ˜˜', 'ðŸ˜ŠðŸ˜Š', ':-)..', '&;:)', 'ðŸš‚', '?!!\"', 'ðŸ’–ðŸ’–:)', '!!!!!!!!!', '??!!', 'Ø¨Ù†Ø¯Ø±', '!*', '(.', '/??\"', '\"...', '~', ':))\"', ':!!!!', ':|', ':))))!!!!!!', '????', 'Å«Å-', ';*!', 'ï½¡âœ§', '()/.', '---&;', '..:-)', '....?', 'â™¥:)', '\"\"', '.\"', '-!', ':)â€\"', '***,', '*...', '(...)', ':-)))))))', 'âœ«', '++', 'ðŸ˜·', '-â…ž', 'ðŸŽµðŸŽµðŸŽµðŸŽµ', \":)'\", 'â˜•â˜•â˜•', 'ðŸ‘ˆðŸ”¥', '/:', '!:))', 'ðŸ‡«ðŸ‡·ðŸ·ðŸ¸â˜€ï¸ðŸ‡«ðŸ‡·ðŸš²ðŸ‡«ðŸ‡·ðŸ·ðŸ¸â˜€ï¸ðŸš²', 'ðŸ‘ˆðŸ’“', ':)))))', '\",', \"''\", 'âœ§', '.....:)', ':):):):)', '______.', 'ðŸ˜„ðŸ˜„?', '........', '!!!!!!', '*-*', ':\")', \"........'\", '!:-)', ':)))))))))))))))', 'âŠ¹', '(&;', '-!!:)', '/,', 'ðŸ‘‰', 'â™«', 'ðŸ‘', 'ðŸŽ‰', '?', '?\"', 'â¤ï¸', ':).', 'ðŸ˜‚ðŸ˜‚ðŸ˜‚', ':)\"\"', 'âœµ', ':-*', '?.', '(:', 'ðŸ˜˜', '?ðŸ˜˜ðŸ˜˜ðŸ˜˜', '-.', 'ðŸ’ƒðŸ’ƒðŸ’ƒ\"', ':-))', 'ðŸ˜´', 'â™¡â™¡â™¡â™¡â™¡', ':-)...', ',,,', '()', 'âš¡', '&;:', 'ðŸ‘', 'ðŸ’…ðŸ½ðŸ’‹', '?:))', '\\U000fe334', 'â¤ï¸â¤ï¸', 'â˜†', 'â€', '..:', '?!?', 'ðŸ‘¯', '::))', '):', ';)', \".'\", \":')\", 'ðŸ˜œ', ';-))', '................', 'ðŸ˜', 'ðŸ˜€', '(..', '...*', ':/', \"'..\", ':::::::', ':-)))', 'Â£Â£,', '():', 'ðŸ˜­â¤ï¸', '\"\\'', '~ðŸ˜Š', '/-', 'Â«--\"', ';-)', 'ðŸ¢', ',,', 'ðŸ˜­', \"'?\", '**', 'ðŸ˜‚', 'Ù„Ù„Ø­ÙŠØ§Ø©,', '......', ':-):-))', ':)..', ':\"', ':-)\"', '!,', 'âˆž', 'ðŸŒ·', \"''!\", '???\"', 'ðŸ’•', \"'!\", 'ðŸŒžðŸŒžðŸŒž', ':---))', '...:', '.:', '_____________', '+', '!:)))', '?:', '..,', ':...', 'â€™', '$', '//*', ':)...', 'âœ¨', ':-):-)', '!!!!!!!!!!!!!!', '\"\",', 'Åº', \"?'\", ':.', '!!..', '.!!.', ':)â™¡', ':-)(-:', ':-))\"', 'ðŸ’œðŸ’œðŸ’œðŸ’œ', ':):', 'â­', 'âˆ—âˆ—', \"'!:)\", 'ðŸ˜ŠðŸ˜ŠðŸ˜Š', \":'-)\", ':)\"', '!!!!:)', '.-..,', '[]', 'Â£', 'ðŸ’—', 'Ã©', 'ðŸ­', 'â†', 'ðŸ’“ðŸš®ðŸ‘¯-', 'ðŸŒŽðŸŒŽâ¤ï¸â¤ï¸â¤ï¸', '??', '-,', 'ðŸ˜', 'ðŸŽ‚', '.:)', '=!', '::)', '.....!!!!', '%!!', 'â™¥â˜…â˜†', ':-))))))))', '.......', '???:', ':)â¤ï¸', '!:', 'Â£......:)', '...'}\n",
            "Total negative similes found: 446\n",
            "Negative similes: {';(', 'à¤«à¤¿à¤°', ':(.', '..-..', '.........', ',,,,&;', 'âœˆï¸', '\"...?', ':(\"', '^^', ':((((', '.........,', 'ðŸ’ž', '?:((', ':-((', '??:(', '(', '(!!)', 'ðŸ’œ', ':(', '):(\"', ':-((((((', '.+)', '$.', 'Â´', '_', 'ðŸœðŸœðŸœðŸœðŸœ', '!!!...', 'â˜º', 'ðŸ˜‚ðŸ˜©ðŸ˜©', '://:(\\\\\\\\\\\\', 'ðŸ’”', '||', \"'.\", '-........:(', ',', 'à¤¸à¥‡à¤', \"'!!\", 'ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜­', '...........:(', 'â¤â¤â¤', 'âœ§ï½¡', 'ðŸ’™', 'â†’', '~~~', '?:(', '....:(', '-_-', ':((((((((((((((((((((((((', \"='(\", '!!&;:(', '...:(;(;(', 'ðŸ’ªðŸ»ðŸ’', ']', 'ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­', '\"-\"', '-', '|', '(:(', '=', 'ðŸ˜‚ðŸ˜‚', '`', '?!?!?!', '_:(Â´à½€`ã€', '{:-(', 'ðŸ˜Ÿ', '....:((', '/./_', '*', '----', '-?', 'à¤§à¥‹à¤–à¤¾........', 'â™›â™›â™›', ':-(((((((((((', '$$,', '!?', '...&;:(', '!!!', \":'(\", ':--((((â€', ':(,', '&;.', '^', ':((((((((', '?ðŸ˜‚ðŸ˜ŠðŸ’˜ðŸ’˜', '.:((', 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â™¥â™¥â™¥â™¥', 'ðŸ˜”ðŸ˜“', ';__;', 'âœ”ï¸', \"':(\", \"'...\", '/', 'â™¡â™¥â™¡â™¥', '...!!', ':(...', ',.', 'ðŸ˜’\"', ':*', 'â€˜â€™', 'â¤', '.............', '???', ':(((((((', 'â€¦.!', '!...', '!!!!', '&;&;', '&;:((', '[', '\"!\"', '???:-(', 'ðŸ’ª', '?!!', 'Ã±', '\"', 'ï¼­ï¼¥', ':((ðŸ’¦â˜”ï¸', '(:-(', '&;&;&;&;&;&;&;&;&;&;', '?&;:(', '?:((((', ';_;', ':(:', ':((((((((((((((((((((((((((((((((', '?.....', 'ðŸ’žðŸŽˆðŸŽ‚ðŸŽ‰ðŸ˜˜', '.....', 'â™¡â™¡â™¡', ':~', '&;', 'ðŸ˜¢', ':??', ':)', 'ðŸš–ðŸ—½ðŸŒƒ', '&;/', 'à¤à¤•', \"',\", ':((((ðŸ˜©', 'â€™,', ':(&;/', 'â€”', '...........', 'ðŸ‘Œ\"', 'ðŸ˜©ðŸ˜©ðŸ˜©ðŸ˜¢', '...â¤', ':(:(:(:(', ':-', '$$$$', '&;.&;', ':(((((((((((', '%', ':(..', ':-(:-(:-(', ':-(.', \":(:'(\", 'ã€‚ã€‚ã€‚_:(Â´Ï‰`ã€', '...,', '!!', ':(((((((((((((((((', 'ðŸ˜˜ðŸŽ‰', 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½', ';(\"', ':-(\"', ':!', 'ðŸ˜ž', '~~', 'ðŸ˜«', 'ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­', 'ðŸ™ðŸ»ðŸŽ‚', ';-;', 'â˜ºðŸ’œ', '!!!!!', \"''.\", '//', ':)):):):(', '!!!!!!!ðŸ’¥âœ¨ðŸ’«', 'ðŸ˜‰\"', 'ðŸ˜˜ðŸ’–', '\":(', ':(\"â€', ':(â¤ï¸â¤ï¸', '--', 'ã€‹ã€‹ã€‹ã€‹ï¼³ï¼¥ï¼¥', 'ðŸ˜˜ðŸ˜˜ðŸ˜˜ðŸ˜˜', ':-(â€', '.....:', ':----((((((', 'ðŸ’›ðŸ’›ðŸ’›ðŸ’›', ').', ':(ðŸ’œ', '---', '&;&;:(', 'ðŸ˜ªðŸ˜ªðŸ˜ª', '.â€¦', \",'\", 'ðŸ˜ºðŸ’’ðŸ’ŽðŸŽ‰', '.????', '?????', 'ðŸ˜­ðŸ‡ºðŸ‡¸', 'ðŸ˜š', '(((', ':-(', '**.', '^_^', '\\U000fe196', '...:(', 'Ã©â€™', ':Â´(', '.....:(', 'â™¡', ':&;', '...?', 'âˆ ):_', ':(((((', 'ðŸ˜¬', 'ðŸ˜ŠðŸ˜ŠðŸ˜Šâ¤ï¸', '&;:(', '(/)', 'ðŸ˜ðŸ˜\"', '!!!!!!!!', ')', '/?:((', 'â€œ', 'ðŸ˜Š', 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â™¥â™¥', '^_^...', 'ï¿½ï¿½ï¿½ï¿½\"', '.', 'â˜ºï¸ðŸ˜‚ðŸ˜', 'â€“', '?:(\"', '.-', \"...'\", 'ðŸ˜•:-(', '!:(', ';;', \"'\", 'Ã¤Ã¸', '.:-(', ':(â€¦', 'ðŸ˜†ðŸ˜†', 'â€¦', '______', '....', ':-(â˜”ï¸', ':(\"\"', ':(?', ':(((((((((((((((', ';', '/(', '!', '..', '-...', '&;--', 'ðŸ‡ºðŸ‡¸', '/?', 'ðŸ˜©', 'ðŸ˜ŠðŸ’ž', '/.', 'ðŸ˜€\"', 'â™¥', '?!', '&;:-(', 'ðŸ˜€ðŸ”«', ':', 'ðŸ˜˜ðŸ˜˜', \"Ã¡'\", ':((((((((((', '?.....:((((', \"..'\", 'Ð¤Ð¾Ñ‚Ð¾ÑÐµÑ‚:', 'ðŸŽ€', ':((((((((((((', '?ðŸ˜Š', '..!!', ':(((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((', 'ðŸ’—ðŸ’—ðŸ’—', '\"...', ':(((((((((', ':(:(', '~', 'ðŸ˜‚ðŸ˜©', ':(((((((((((((', \":'\", ':|', 'ã€‹ã€‹ã€‹ã€‹', '????', 'ØªÙƒÙÙ‰', 'ï½¡âœ§', '\"\\'\"', ':(((', 'ðŸ˜­ðŸ˜­', '.:(', 'ðŸ’•ðŸ’•ðŸ’•ðŸ’•', '............./', 'ðŸ˜‹', '\"\"', ':(;(', '.\"', '-!', 'ðŸ˜¿ðŸ’“ðŸ’“ðŸ’•ðŸ˜šðŸ˜šðŸ˜šðŸ˜šðŸ˜šâ¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸', \"!!!'\", 'ðŸ˜Šâ€', 'ðŸ˜”ðŸ˜”ðŸ˜”', '?!?!!', ':((((((((((((((((((((((((((((((((((((((((((((((((((((((((((', '++', ':-(((', '^__________^', ':((', 'ðŸ˜‚ðŸ’ž', 'ðŸ˜·', ':-(((((((((', '\\\\\"', 'ï¼ _', '......:((', 'ðŸ˜„ðŸ‘', '\",', \"''\", 'ðŸ˜“', 'ï¼ __', ':(;', '?!?!?:(:(:(:(:(', '(,', 'ðŸ¹ðŸ¸ðŸ¹', '........', '!!!!!!', ':..', 'ðŸ’‹â¤ðŸ’‹â¤', '..:-(', '!!!!!!!!!!!!', 'Â¿', 'ðŸ˜’ðŸ˜’', 'ðŸ‘‰', '??!', ':(:(:(', 'â™«', '?', '?\"', 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â™¥â™¥â™¥', 'â¤ï¸', 'ðŸ˜©ðŸ˜­ðŸµðŸŒ¸', 'ðŸ’–', '((((', 'ðŸ˜‚ðŸ˜‚ðŸ˜‚', 'ðŸŒžðŸ’ƒðŸ½ðŸ¹', 'ðŸ‘†ðŸ»ðŸ‘†ðŸ»', 'ðŸ˜', ':-*', '(:', 'ðŸ˜˜', '-.', 'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â™¥', ':-((((', ',,,', '()', ':(ðŸ˜©ðŸ’–', 'â€', 'ðŸ˜­ðŸ˜­ðŸ˜­', \"....'\", \"~'\", \".'\", '--:', '...:(...', 'ðŸ˜”', 'ðŸ˜ðŸ˜', 'ðŸ˜', 'ðŸ˜€', ':((((((((((((((', '...*', ':/', 'â¤ï¸.', 'â˜¼', ':((:(', '\"\\'', '),', ';-)', ',,', '!(:â™¡â™¡', 'ðŸ˜­', '**', 'ðŸ˜‚', \"'?'\", '......', '!!!).', ':(!', 'ðŸ‘£', '..:(', '???\"', 'ðŸ˜¢ðŸ˜µðŸ˜­', 'ðŸ’•', 'à¤¬à¤¾à¤°', '&;:(((((((((', '....\"\"', '...:', '+=', ':\\\\', '.:', '\"...*..', 'ðŸ’›', '+', 'ðŸµâ˜•ï¸', ':()', ':(â€', 'ðŸš™ðŸ’š', ':...', 'â€™', '$', 'â™¥â™¥â™¥', \"('\", '(+)', '&;&;&;\"', '??!?:(', '*%*', '!!!!!!!!!!!!!!', '...:-(', ':-(:-(:-(....', ':((*', 'ðŸ˜‚ðŸ˜‚ðŸ˜‚\"', ':-(((((((', ':-(((((', 'â€¦.', 'ðŸ˜­ðŸ‘‘', '[]', 'ðŸ˜­ðŸ’–', 'Â£', 'ðŸ’—', 'Ã©', ':((((((', '.â€', '??', '-,', 'ðŸ’œðŸ’œðŸ’œ', 'ðŸ˜¢\"', 'ðŸ˜', '*\"', '...???', ':(!!!', '.......', '.%', 'Ä°', ':-\\\\', \"&;'\", '&;_&;', '...'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see change in positive samples \n",
        "for index in random_indexs:\n",
        "  # print random tweet\n",
        "  print('Tweet number:',index,':',cleaned_positive_tweets[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhzGC7U9lThf",
        "outputId": "213f97af-a0ce-4844-d8f0-9ea9840f20f2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet number: 327 : ['Gratefulness', 'Happiness', 'positive', 'Post', 'three', 'things', 'grateful', 'today', 'comments']\n",
            "Tweet number: 554 : ['thnks', 'progrmr']\n",
            "Tweet number: 4084 : ['Not', 'sure', 'would', 'good', 'thing', 'bottom', 'daring', 'say', 'Miss', 'gon', 'stubborn', 'mouth', 'soaping', 'positive']\n",
            "Tweet number: 61 : ['follow', 'amp', 'follow', 'back', 'positive']\n",
            "Tweet number: 3728 : ['special', 'Acha', 'banda', 'tweets', 'bhi', 'achi', 'krta', 'Handsome', 'bhi', 'Must', 'follow', 'positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see change in negative samples \n",
        "for index in random_indexs:\n",
        "  # print random tweet\n",
        "  print('Tweet number:',index,':',cleaned_negative_tweets[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmcnomJ9leKR",
        "outputId": "b54361a7-8df0-4227-db1f-b5247d63c09b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet number: 327 : ['Such', 'stressful', 'upsetting', 'day', 'yesterday', 'government', 'sucks', 'negative']\n",
            "Tweet number: 554 : ['ruins', 'know', 'next', 'day', 'wake', 'early', 'morning', 'negative']\n",
            "Tweet number: 4084 : ['Wish', 'buy', 'Bangtan', 'merch', 'negative']\n",
            "Tweet number: 61 : ['Well', 'sure', 'work', 'day', 'negative']\n",
            "Tweet number: 3728 : ['hii', 'vin', 'plss', 'rply', 'tweet', 'negative']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pre-process data "
      ],
      "metadata": {
        "id": "DCW1Z2S5w5Hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make stem and lemmatize words (positive data)\n",
        "cleaned_positive_tweets=wordCleaner(cleaned_positive_tweets)\n",
        "# make stem and lemmatize words (positive data)\n",
        "cleaned_negative_tweets=wordCleaner(cleaned_negative_tweets)"
      ],
      "metadata": {
        "id": "LOWeSbGEw7Kp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see change in positive samples \n",
        "for index in random_indexs:\n",
        "  # print random tweet\n",
        "  print('Tweet number:',index,':',cleaned_positive_tweets[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4O9tJTjxR5K",
        "outputId": "5a084d6f-1ae9-4983-af12-aec1fe940074"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet number: 4427 : ['overwhelm', 'groupmat', 'thank', 'group', 'get', 'recit', 'posit']\n",
            "Tweet number: 1657 : ['hell', 'yes', 'will', 'donington', 'park', 'hop', 'see', 'team', 'season', 'our', 'posit']\n",
            "Tweet number: 4433 : ['mont', 'doa', 'header', 'posit']\n",
            "Tweet number: 1503 : ['check', 'place', 'avail', 'colour', 'aggreg', 'work', 'tend', 'fade']\n",
            "Tweet number: 4968 : ['thank', 'mention', 'happi', 'friday', 'posit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see change in negative samples \n",
        "for index in random_indexs:\n",
        "  # print random tweet\n",
        "  print('Tweet number:',index,':',cleaned_negative_tweets[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUxo0Dfmxk1M",
        "outputId": "f862fc1b-fc11-44e0-fa9b-d9fded4e0f17"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet number: 327 : ['such', 'stress', 'upset', 'day', 'yesterday', 'govern', 'suck', 'negat']\n",
            "Tweet number: 554 : ['ruin', 'know', 'next', 'day', 'wake', 'earli', 'morn', 'negat']\n",
            "Tweet number: 4084 : ['wish', 'buy', 'bangtan', 'merch', 'negat']\n",
            "Tweet number: 61 : ['well', 'sure', 'work', 'day', 'negat']\n",
            "Tweet number: 3728 : ['hii', 'vin', 'plss', 'rpli', 'tweet', 'negat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poistiveGenerator=getGenerator(cleaned_positive_tweets)\n",
        "print(f'type of postive_word_gen_ -> {poistiveGenerator}:',type(poistiveGenerator))\n",
        "# make frequency dict \n",
        "from nltk import FreqDist\n",
        "# make postive words frequency dictionary \n",
        "positive_words_frequency=FreqDist(poistiveGenerator)\n",
        "# see -> frequency dict \n",
        "print(positive_words_frequency)\n",
        "print('Frequency of top 10 most common elements [positive]:\\n',positive_words_frequency.most_common(10),sep='')\n",
        "type(positive_words_frequency) # to see resutl -> postive_freq_dict_ -> return dict "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j29lxDNzJZo",
        "outputId": "4e10e3c5-980d-4277-f0e0-ce9bb0bdc846"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type of postive_word_gen_ -> <generator object getGenerator at 0x7f66c63564d0>: <class 'generator'>\n",
            "<FreqDist with 5227 samples and 29482 outcomes>\n",
            "Frequency of top 10 most common elements [positive]:\n",
            "[('posit', 4509), ('thank', 646), ('follow', 443), ('love', 399), ('good', 288), ('get', 273), ('day', 248), ('like', 233), ('happi', 208), ('see', 196)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.probability.FreqDist"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negativeGenerator=getGenerator(cleaned_negative_tweets)\n",
        "print(f'type of postive_word_gen_ -> {negativeGenerator}:',type(negativeGenerator))\n",
        "# make frequency dict \n",
        "from nltk import FreqDist\n",
        "\n",
        "# make postive words frequency dictionary \n",
        "negative_words_frequency=FreqDist(negativeGenerator)\n",
        "# see -> frequency dict \n",
        "print(negative_words_frequency)\n",
        "print('Frequency of top 10 most common elements [negative]:\\n',negative_words_frequency.most_common(10),sep='')\n",
        "type(negative_words_frequency) # to see resutl -> postive_freq_dict_ -> return dict "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW1I7B_uxlJ0",
        "outputId": "5e5c05a6-4dfd-401b-b2ed-ced1e4f74dc8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type of postive_word_gen_ -> <generator object getGenerator at 0x7f66be3744d0>: <class 'generator'>\n",
            "<FreqDist with 5349 samples and 28399 outcomes>\n",
            "Frequency of top 10 most common elements [negative]:\n",
            "[('negat', 4636), ('get', 330), ('miss', 304), ('pleas', 275), ('follow', 263), ('like', 223), ('time', 166), ('feel', 158), ('love', 151), ('day', 150)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.probability.FreqDist"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make dataset for model (modeled data)\n",
        "poistive_words4Data=getModeledTokens(cleaned_positive_tweets)\n",
        "negative_words4Data=getModeledTokens(cleaned_negative_tweets)\n",
        "# see (first sample)\n",
        "print('Sample: ',next(getModeledTokens(cleaned_positive_tweets)))\n",
        "\n",
        "# make dataset - \n",
        "positive_dataset=[(tweet_dict,'Positive') for tweet_dict in poistive_words4Data]\n",
        "negative_dataset=[(tweet_dict,'Negative') for tweet_dict in negative_words4Data]\n",
        "# merge dataset \n",
        "dataset=positive_dataset+negative_dataset\n",
        "print('Size of dataset:',len(dataset)) \n",
        "# split data set -> 70:30 \n",
        "from sklearn.model_selection import train_test_split\n",
        "# split data\n",
        "train_set,test_set=train_test_split(dataset,test_size=0.30)\n",
        "# see size \n",
        "len(train_set),len(test_set) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4S64pxhzou6",
        "outputId": "5fc72446-e8a5-4f7c-f911-ae22353df022"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample:  {'top': True, 'engag': True, 'member': True, 'communiti': True, 'week': True, 'posit': True}\n",
            "Size of dataset: 10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7000, 3000)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see first sample (final data)\n",
        "[(tweet_dict,'Positive') for tweet_dict in getModeledTokens(cleaned_positive_tweets)][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5I8A1vn1RoL",
        "outputId": "fd82f7ca-8a52-44ec-e48a-0756e188f3a9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'communiti': True,\n",
              "  'engag': True,\n",
              "  'member': True,\n",
              "  'posit': True,\n",
              "  'top': True,\n",
              "  'week': True},\n",
              " 'Positive')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKkxzghUXV-d"
      },
      "source": [
        "## training and testing model - By using NaiveBayesClassifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um-IOKZNUQeR",
        "outputId": "cb5c7be8-230f-4391-8cf8-d6749f778961"
      },
      "source": [
        "# import module \n",
        "from nltk import classify,NaiveBayesClassifier\n",
        "# train maodel \n",
        "classifier_=NaiveBayesClassifier.train(train_set)\n",
        "# test model \n",
        "print('Accuracy score:',round(classify.accuracy(classifier_,test_set)*100,3))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score: 95.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2oFK3VUoQAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d5405ff-ca0f-4029-d30e-9fb7b7917ddb"
      },
      "source": [
        "# classifier 10 most informative features\n",
        "classifier_.show_most_informative_features(10)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "                   negat = True           Negati : Positi =   2116.7 : 1.0\n",
            "                   posit = True           Positi : Negati =   1990.9 : 1.0\n",
            "                     bam = True           Positi : Negati =     21.9 : 1.0\n",
            "                     sad = True           Negati : Positi =     20.3 : 1.0\n",
            "                     too = True           Negati : Positi =     16.5 : 1.0\n",
            "                 appreci = True           Positi : Negati =     15.4 : 1.0\n",
            "                     via = True           Positi : Negati =     15.4 : 1.0\n",
            "              bestfriend = True           Positi : Negati =     15.1 : 1.0\n",
            "                  awesom = True           Positi : Negati =     13.6 : 1.0\n",
            "                 definit = True           Positi : Negati =     12.8 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGBl-rJEX8hi"
      },
      "source": [
        "## testing on hindi input"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def conversion(get_inp):\n",
        "  from googletrans import Translator\n",
        "  translator = Translator()\n",
        "  get_inp = translator.translate(text = get_inp,dest = 'en',src = 'hi')\n",
        "  get_inp = get_inp.text\n",
        "  return [get_inp]"
      ],
      "metadata": {
        "id": "D9N-Yo3I9aZe"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample postive sentence \n",
        "inp=input(\"Enter text in hindi\")\n",
        "custom_tweet=conversion(inp)\n",
        "cleaned_custom_tweet=removeStopWords(specialCharaterRemover(tagRemover(custom_tweet),positive_similes,\n",
        "                                                            to_prevent_positive,positive_similes_code))\n",
        "# preprocessing - \n",
        "custom_tokens=wordCleaner(cleaned_custom_tweet)\n",
        "\n",
        "# print results ## Note classifier_ takes only single sample at a time (not like sklearn classifiers) \n",
        "print(custom_tweet,' <- type:',classifier_.classify([dict([token,True] for token in sample) for sample in custom_tokens][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwmfV0_t3af8",
        "outputId": "7fdf1d6e-7938-473e-d8e6-6cd4ad4e3116"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text in hindià¤­à¤¾à¤°à¤¤ à¤¸à¥à¤‚à¤¦à¤° à¤¹à¥ˆ\n",
            "['India is beautiful']  <- type: Positive\n"
          ]
        }
      ]
    }
  ]
}